#!/usr/bin/env python3
"""
è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬å™¨
ç›£æ§è¨“ç·´éç¨‹ä¸­çš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼Œæª¢æ¸¬æ½›åœ¨çš„è¨˜æ†¶é«”æ´©æ¼å•é¡Œ
"""

import gc
import psutil
import torch
import time
import threading
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np


@dataclass
class MemorySnapshot:
    """è¨˜æ†¶é«”å¿«ç…§"""
    timestamp: float
    cpu_memory_mb: float
    gpu_memory_mb: float
    gpu_allocated_mb: float
    gpu_cached_mb: float
    num_tensors: int
    epoch: Optional[int] = None
    step: Optional[int] = None


class MemoryLeakDetector:
    """è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬å™¨"""
    
    def __init__(self, 
                 check_interval: float = 10.0,  # æª¢æŸ¥é–“éš”ï¼ˆç§’ï¼‰
                 warning_threshold_mb: float = 1000,  # è­¦å‘Šé–¾å€¼ï¼ˆMBï¼‰
                 critical_threshold_mb: float = 2000,  # å±éšªé–¾å€¼ï¼ˆMBï¼‰
                 max_snapshots: int = 1000):  # æœ€å¤§å¿«ç…§æ•¸é‡
        self.check_interval = check_interval
        self.warning_threshold = warning_threshold_mb
        self.critical_threshold = critical_threshold_mb
        self.max_snapshots = max_snapshots
        
        self.snapshots: List[MemorySnapshot] = []
        self.monitoring = False
        self.monitor_thread = None
        self.start_time = time.time()
        
        # åŸºç·šè¨˜æ†¶é«”ä½¿ç”¨é‡
        self.baseline_cpu = None
        self.baseline_gpu = None
        
    def start_monitoring(self):
        """é–‹å§‹ç›£æ§"""
        if self.monitoring:
            return
            
        self.monitoring = True
        self.start_time = time.time()
        
        # è¨˜éŒ„åŸºç·š
        snapshot = self._take_snapshot()
        self.baseline_cpu = snapshot.cpu_memory_mb
        if torch.cuda.is_available():
            self.baseline_gpu = snapshot.gpu_memory_mb
            
        self.snapshots.append(snapshot)
        
        # å•Ÿå‹•ç›£æ§ç·šç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        
        print(f"ğŸ” è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬å™¨å·²å•Ÿå‹•")
        print(f"   åŸºç·šCPUè¨˜æ†¶é«”: {self.baseline_cpu:.1f} MB")
        if self.baseline_gpu:
            print(f"   åŸºç·šGPUè¨˜æ†¶é«”: {self.baseline_gpu:.1f} MB")
    
    def stop_monitoring(self):
        """åœæ­¢ç›£æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        print("ğŸ” è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬å™¨å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring:
            try:
                snapshot = self._take_snapshot()
                self.snapshots.append(snapshot)
                
                # é™åˆ¶å¿«ç…§æ•¸é‡
                if len(self.snapshots) > self.max_snapshots:
                    self.snapshots.pop(0)
                
                # æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼
                self._check_memory_leak(snapshot)
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                print(f"âš ï¸ è¨˜æ†¶é«”ç›£æ§éŒ¯èª¤: {e}")
                time.sleep(self.check_interval)
    
    def _take_snapshot(self) -> MemorySnapshot:
        """æ‹æ”è¨˜æ†¶é«”å¿«ç…§"""
        # CPUè¨˜æ†¶é«”
        process = psutil.Process()
        cpu_memory_mb = process.memory_info().rss / 1024 / 1024
        
        # GPUè¨˜æ†¶é«”
        gpu_memory_mb = 0
        gpu_allocated_mb = 0
        gpu_cached_mb = 0
        num_tensors = 0
        
        if torch.cuda.is_available():
            gpu_memory_mb = torch.cuda.memory_allocated() / 1024 / 1024
            gpu_allocated_mb = torch.cuda.memory_allocated() / 1024 / 1024
            gpu_cached_mb = torch.cuda.memory_reserved() / 1024 / 1024
            
            # è¨ˆç®—tensoræ•¸é‡
            for obj in gc.get_objects():
                if torch.is_tensor(obj) and obj.is_cuda:
                    num_tensors += 1
        
        return MemorySnapshot(
            timestamp=time.time() - self.start_time,
            cpu_memory_mb=cpu_memory_mb,
            gpu_memory_mb=gpu_memory_mb,
            gpu_allocated_mb=gpu_allocated_mb,
            gpu_cached_mb=gpu_cached_mb,
            num_tensors=num_tensors
        )
    
    def _check_memory_leak(self, snapshot: MemorySnapshot):
        """æª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼"""
        if len(self.snapshots) < 5:  # éœ€è¦è¶³å¤ çš„æ­·å²æ•¸æ“š
            return
        
        # è¨ˆç®—è¨˜æ†¶é«”å¢é•·è¶¨å‹¢
        recent_snapshots = self.snapshots[-5:]
        cpu_growth = snapshot.cpu_memory_mb - recent_snapshots[0].cpu_memory_mb
        gpu_growth = snapshot.gpu_memory_mb - recent_snapshots[0].gpu_memory_mb
        
        # CPUè¨˜æ†¶é«”æ´©æ¼æª¢æŸ¥
        if cpu_growth > self.warning_threshold:
            if cpu_growth > self.critical_threshold:
                print(f"ğŸš¨ åš´é‡CPUè¨˜æ†¶é«”æ´©æ¼è­¦å‘Š!")
                print(f"   éå» {len(recent_snapshots)} æ¬¡æª¢æŸ¥å¢é•·: {cpu_growth:.1f} MB")
                print(f"   ç•¶å‰ä½¿ç”¨é‡: {snapshot.cpu_memory_mb:.1f} MB")
                self._suggest_cpu_fixes()
            else:
                print(f"âš ï¸ CPUè¨˜æ†¶é«”å¢é•·è­¦å‘Š: +{cpu_growth:.1f} MB")
        
        # GPUè¨˜æ†¶é«”æ´©æ¼æª¢æŸ¥
        if torch.cuda.is_available() and gpu_growth > self.warning_threshold:
            if gpu_growth > self.critical_threshold:
                print(f"ğŸš¨ åš´é‡GPUè¨˜æ†¶é«”æ´©æ¼è­¦å‘Š!")
                print(f"   éå» {len(recent_snapshots)} æ¬¡æª¢æŸ¥å¢é•·: {gpu_growth:.1f} MB")
                print(f"   ç•¶å‰ä½¿ç”¨é‡: {snapshot.gpu_memory_mb:.1f} MB")
                print(f"   GPU tensoræ•¸é‡: {snapshot.num_tensors}")
                self._suggest_gpu_fixes()
            else:
                print(f"âš ï¸ GPUè¨˜æ†¶é«”å¢é•·è­¦å‘Š: +{gpu_growth:.1f} MB")
    
    def _suggest_cpu_fixes(self):
        """å»ºè­°CPUè¨˜æ†¶é«”ä¿®å¾©æ–¹æ³•"""
        print("ğŸ’¡ CPUè¨˜æ†¶é«”æ´©æ¼ä¿®å¾©å»ºè­°:")
        print("   1. æª¢æŸ¥æ˜¯å¦æœ‰æœªé‡‹æ”¾çš„å¤§å‹æ•¸æ“šçµæ§‹")
        print("   2. ç¢ºä¿validation_step_outputså’Œtest_step_outputsè¢«æ­£ç¢ºæ¸…ç†")
        print("   3. æ¸›å°‘DataLoaderçš„num_workers")
        print("   4. ç¦ç”¨persistent_workers")
        print("   5. èª¿ç”¨gc.collect()å¼·åˆ¶åƒåœ¾å›æ”¶")
    
    def _suggest_gpu_fixes(self):
        """å»ºè­°GPUè¨˜æ†¶é«”ä¿®å¾©æ–¹æ³•"""
        print("ğŸ’¡ GPUè¨˜æ†¶é«”æ´©æ¼ä¿®å¾©å»ºè­°:")
        print("   1. ç¢ºä¿tensorä½¿ç”¨.detach().cpu()ç§»åˆ°CPU")
        print("   2. èª¿ç”¨torch.cuda.empty_cache()æ¸…ç†GPUç·©å­˜")
        print("   3. æª¢æŸ¥æ˜¯å¦æœ‰tensoræ²’æœ‰æ­£ç¢ºé‡‹æ”¾")
        print("   4. æ¸›å°‘batch_size")
        print("   5. ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´(precision=16)")
    
    def manual_check(self, epoch: Optional[int] = None, step: Optional[int] = None):
        """æ‰‹å‹•æª¢æŸ¥è¨˜æ†¶é«”ç‹€æ…‹"""
        snapshot = self._take_snapshot()
        snapshot.epoch = epoch
        snapshot.step = step
        self.snapshots.append(snapshot)
        
        print(f"ğŸ“Š è¨˜æ†¶é«”ç‹€æ…‹æª¢æŸ¥ (Epoch {epoch}, Step {step}):")
        print(f"   CPUè¨˜æ†¶é«”: {snapshot.cpu_memory_mb:.1f} MB")
        if torch.cuda.is_available():
            print(f"   GPUè¨˜æ†¶é«”: {snapshot.gpu_memory_mb:.1f} MB")
            print(f"   GPUç·©å­˜: {snapshot.gpu_cached_mb:.1f} MB")
            print(f"   GPU Tensoræ•¸é‡: {snapshot.num_tensors}")
        
        # èˆ‡åŸºç·šæ¯”è¼ƒ
        if self.baseline_cpu:
            cpu_diff = snapshot.cpu_memory_mb - self.baseline_cpu
            print(f"   CPUå¢é•·: {cpu_diff:+.1f} MB")
            
        if self.baseline_gpu and torch.cuda.is_available():
            gpu_diff = snapshot.gpu_memory_mb - self.baseline_gpu
            print(f"   GPUå¢é•·: {gpu_diff:+.1f} MB")
    
    def force_cleanup(self):
        """å¼·åˆ¶æ¸…ç†è¨˜æ†¶é«”"""
        print("ğŸ§¹ åŸ·è¡Œå¼·åˆ¶è¨˜æ†¶é«”æ¸…ç†...")
        
        # Pythonåƒåœ¾å›æ”¶
        collected = gc.collect()
        print(f"   Python GCå›æ”¶å°è±¡: {collected}")
        
        # GPUè¨˜æ†¶é«”æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   GPUç·©å­˜å·²æ¸…ç†")
        
        # æ‹æ”æ¸…ç†å¾Œå¿«ç…§
        snapshot = self._take_snapshot()
        print(f"   æ¸…ç†å¾ŒCPUè¨˜æ†¶é«”: {snapshot.cpu_memory_mb:.1f} MB")
        if torch.cuda.is_available():
            print(f"   æ¸…ç†å¾ŒGPUè¨˜æ†¶é«”: {snapshot.gpu_memory_mb:.1f} MB")
    
    def generate_report(self, save_path: str = "memory_leak_report.png"):
        """ç”Ÿæˆè¨˜æ†¶é«”ä½¿ç”¨å ±å‘Š"""
        if len(self.snapshots) < 2:
            print("âš ï¸ è¨˜æ†¶é«”å¿«ç…§æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Š")
            return
        
        timestamps = [s.timestamp / 60 for s in self.snapshots]  # è½‰æ›ç‚ºåˆ†é˜
        cpu_memory = [s.cpu_memory_mb for s in self.snapshots]
        
        plt.figure(figsize=(12, 8))
        
        # CPUè¨˜æ†¶é«”åœ–
        plt.subplot(2, 1, 1)
        plt.plot(timestamps, cpu_memory, 'b-', linewidth=2, label='CPU Memory')
        if self.baseline_cpu:
            plt.axhline(y=self.baseline_cpu, color='b', linestyle='--', alpha=0.7, label='Baseline')
        plt.xlabel('æ™‚é–“ (åˆ†é˜)')
        plt.ylabel('CPUè¨˜æ†¶é«” (MB)')
        plt.title('CPUè¨˜æ†¶é«”ä½¿ç”¨è¶¨å‹¢')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # GPUè¨˜æ†¶é«”åœ–
        if torch.cuda.is_available() and any(s.gpu_memory_mb > 0 for s in self.snapshots):
            plt.subplot(2, 1, 2)
            gpu_memory = [s.gpu_memory_mb for s in self.snapshots]
            gpu_cached = [s.gpu_cached_mb for s in self.snapshots]
            
            plt.plot(timestamps, gpu_memory, 'r-', linewidth=2, label='GPU Allocated')
            plt.plot(timestamps, gpu_cached, 'orange', linewidth=2, label='GPU Cached')
            if self.baseline_gpu:
                plt.axhline(y=self.baseline_gpu, color='r', linestyle='--', alpha=0.7, label='Baseline')
            plt.xlabel('æ™‚é–“ (åˆ†é˜)')
            plt.ylabel('GPUè¨˜æ†¶é«” (MB)')
            plt.title('GPUè¨˜æ†¶é«”ä½¿ç”¨è¶¨å‹¢')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è¨˜æ†¶é«”ä½¿ç”¨å ±å‘Šå·²ä¿å­˜: {save_path}")
    
    def get_summary(self) -> Dict:
        """ç²å–è¨˜æ†¶é«”ä½¿ç”¨æ‘˜è¦"""
        if not self.snapshots:
            return {}
        
        latest = self.snapshots[-1]
        
        summary = {
            "monitoring_duration_minutes": latest.timestamp / 60,
            "current_cpu_memory_mb": latest.cpu_memory_mb,
            "current_gpu_memory_mb": latest.gpu_memory_mb,
            "current_gpu_tensors": latest.num_tensors,
            "total_snapshots": len(self.snapshots)
        }
        
        if self.baseline_cpu:
            summary["cpu_memory_growth_mb"] = latest.cpu_memory_mb - self.baseline_cpu
            
        if self.baseline_gpu:
            summary["gpu_memory_growth_mb"] = latest.gpu_memory_mb - self.baseline_gpu
        
        return summary


# å…¨å±€æª¢æ¸¬å™¨å¯¦ä¾‹
_global_detector = None

def get_memory_detector() -> MemoryLeakDetector:
    """ç²å–å…¨å±€è¨˜æ†¶é«”æª¢æ¸¬å™¨"""
    global _global_detector
    if _global_detector is None:
        _global_detector = MemoryLeakDetector()
    return _global_detector

def start_memory_monitoring():
    """å•Ÿå‹•è¨˜æ†¶é«”ç›£æ§"""
    detector = get_memory_detector()
    detector.start_monitoring()

def stop_memory_monitoring():
    """åœæ­¢è¨˜æ†¶é«”ç›£æ§"""
    detector = get_memory_detector()
    detector.stop_monitoring()

def check_memory(epoch: Optional[int] = None, step: Optional[int] = None):
    """æª¢æŸ¥è¨˜æ†¶é«”ç‹€æ…‹"""
    detector = get_memory_detector()
    detector.manual_check(epoch, step)

def cleanup_memory():
    """æ¸…ç†è¨˜æ†¶é«”"""
    detector = get_memory_detector()
    detector.force_cleanup()

def generate_memory_report(save_path: str = "memory_leak_report.png"):
    """ç”Ÿæˆè¨˜æ†¶é«”å ±å‘Š"""
    detector = get_memory_detector()
    detector.generate_report(save_path)


if __name__ == "__main__":
    # æ¸¬è©¦è¨˜æ†¶é«”æª¢æ¸¬å™¨
    print("ğŸ§ª æ¸¬è©¦è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬å™¨...")
    
    detector = MemoryLeakDetector(check_interval=2.0)
    detector.start_monitoring()
    
    try:
        # æ¨¡æ“¬è¨˜æ†¶é«”ä½¿ç”¨
        tensors = []
        for i in range(5):
            print(f"å‰µå»ºtensor {i+1}...")
            if torch.cuda.is_available():
                tensor = torch.randn(1000, 1000).cuda()
            else:
                tensor = torch.randn(1000, 1000)
            tensors.append(tensor)
            
            detector.manual_check(epoch=i)
            time.sleep(3)
        
        # æ¸…ç†
        print("æ¸…ç†tensors...")
        del tensors
        detector.force_cleanup()
        detector.manual_check(epoch=5)
        
        time.sleep(5)
        
    finally:
        detector.stop_monitoring()
        detector.generate_report("test_memory_report.png")
        
        summary = detector.get_summary()
        print("\nğŸ“Š è¨˜æ†¶é«”ä½¿ç”¨æ‘˜è¦:")
        for key, value in summary.items():
            print(f"   {key}: {value}") 