#!/usr/bin/env python
"""
ç¨ç«‹ç¨‹å¼ï¼šè¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦ç”Ÿæˆé©—è­‰é›†æ•£é»åœ–
ä½¿ç”¨æ–¹æ³•: python generate_validation_plots.py --checkpoint_path outputs_safe/run_20250527_044827/checkpoints/cardiac_dreamer-epoch=45-val_main_task_loss=0.4943.ckpt
"""

import os
import sys
import argparse
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from sklearn.metrics import r2_score
import json
from datetime import datetime

# Add project root to path
current_file_dir = os.path.dirname(os.path.abspath(__file__))
if current_file_dir not in sys.path:
    sys.path.insert(0, current_file_dir)

# Import model and dataset
from src.models.system import get_cardiac_dreamer_system
from src.data import CrossPatientTransitionsDataset, get_patient_splits, get_custom_patient_splits_no_test


def load_model_from_checkpoint(checkpoint_path: str):
    """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
    print(f"Loading model from: {checkpoint_path}")
    
    # è¼‰å…¥ checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # å˜—è©¦å¾ checkpoint ä¸­ç²å–è¶…åƒæ•¸
    if 'hyper_parameters' in checkpoint:
        hparams = checkpoint['hyper_parameters']
        print(f"ğŸ“Š Found hyperparameters in checkpoint: {list(hparams.keys())}")
        
        # ä½¿ç”¨ checkpoint ä¸­çš„é…ç½®
        model = get_cardiac_dreamer_system(
            token_type=hparams.get("token_type", "channel"),
            d_model=hparams.get("d_model", 768),
            nhead=hparams.get("nhead", 12),
            num_layers=hparams.get("num_layers", 6),
            feature_dim=hparams.get("feature_dim", 49),
            lr=hparams.get("lr", 1e-4),
            weight_decay=hparams.get("weight_decay", 1e-5),
            lambda_t2_action=hparams.get("lambda_t2_action", 1.0),
            smooth_l1_beta=hparams.get("smooth_l1_beta", 1.0),
            use_flash_attn=hparams.get("use_flash_attn", False),
            primary_task_only=hparams.get("primary_task_only", False)
        )
    else:
        print("âš ï¸ No hyperparameters found in checkpoint, using default configuration")
        # ä½¿ç”¨é è¨­é…ç½®
        model = get_cardiac_dreamer_system(
            token_type="channel",
            d_model=768,
            nhead=12,
            num_layers=6,
            feature_dim=49,
            lr=1e-4,
            weight_decay=1e-5,
            lambda_t2_action=1.0,
            smooth_l1_beta=1.0,
            use_flash_attn=False,
            primary_task_only=False
        )
    
    # è¼‰å…¥æ¨¡å‹æ¬Šé‡
    try:
        model.load_state_dict(checkpoint['state_dict'])
        print("âœ… Model loaded successfully!")
    except Exception as e:
        print(f"âŒ Error loading model state dict: {e}")
        print("ğŸ”§ Trying to load with strict=False...")
        model.load_state_dict(checkpoint['state_dict'], strict=False)
        print("âœ… Model loaded with some missing/unexpected keys")
    
    model.eval()
    return model


def create_validation_dataset(data_dir: str = "data/processed", use_custom_split: bool = False):
    """å‰µå»ºé©—è­‰é›†"""
    print(f"Creating validation dataset from: {data_dir}")
    
    # é¸æ“‡åˆ†å‰²æ–¹æ³•
    if use_custom_split:
        print("Using custom split: patients 1-5 as validation set")
        train_patients, val_patients, test_patients = get_custom_patient_splits_no_test(data_dir)
    else:
        print("Using automatic patient splits")
        train_patients, val_patients, test_patients = get_patient_splits(data_dir)
    
    # è¨­å®šåœ–åƒè½‰æ›
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])
    
    # å‰µå»ºé©—è­‰é›†
    val_dataset = CrossPatientTransitionsDataset(
        data_dir=data_dir,
        transform=transform,
        split="val",
        train_patients=train_patients,
        val_patients=val_patients,
        test_patients=test_patients,
        small_subset=False,
        normalize_actions=True
    )
    
    print(f"âœ… Validation dataset created: {len(val_dataset)} samples")
    print(f"ğŸ“Š Validation patients: {val_dataset.get_patient_stats()}")
    
    return val_dataset


def generate_predictions(model, val_loader, device):
    """Generate predictions on validation set"""
    model.eval()
    
    all_predictions = []
    all_targets = []
    
    print("Generating predictions...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            if batch_idx % 10 == 0:
                print(f"Processing batch {batch_idx + 1}/{len(val_loader)}")
            
            image_t1, a_hat_t1_to_t2_gt, at1_6dof_gt, at2_6dof_gt = batch
            
            # ç§»å‹•åˆ°è¨­å‚™
            image_t1 = image_t1.to(device)
            a_hat_t1_to_t2_gt = a_hat_t1_to_t2_gt.to(device)
            at1_6dof_gt = at1_6dof_gt.to(device)
            
            # å‰å‘å‚³æ’­
            outputs = model(image_t1, a_hat_t1_to_t2_gt)
            at1_pred = outputs['predicted_action_composed']  # ä¸»ä»»å‹™é æ¸¬
            
            # ğŸ¯ çµ±ä¸€åæ­£è¦åŒ–ï¼šæ‰€æœ‰é æ¸¬å’Œç›®æ¨™éƒ½ä½¿ç”¨ç›¸åŒçš„çµ±è¨ˆé‡
            # å¾é©—è­‰é›†ç²å–æ­£è¦åŒ–çµ±è¨ˆé‡
            if hasattr(val_loader.dataset, 'action_mean') and hasattr(val_loader.dataset, 'action_std'):
                action_mean = torch.tensor(val_loader.dataset.action_mean, device=device)
                action_std = torch.tensor(val_loader.dataset.action_std, device=device)
                
                # åæ­£è¦åŒ–é æ¸¬å€¼å’Œç›®æ¨™å€¼
                at1_pred_denorm = at1_pred * action_std + action_mean
                at1_target_denorm = at1_6dof_gt * action_std + action_mean
            else:
                print("Warning: No normalization stats found, using raw values")
                at1_pred_denorm = at1_pred
                at1_target_denorm = at1_6dof_gt
            
            # æ”¶é›†çµæœï¼ˆä½¿ç”¨åæ­£è¦åŒ–å¾Œçš„å€¼ï¼‰
            all_predictions.append(at1_pred_denorm.cpu().numpy())
            all_targets.append(at1_target_denorm.cpu().numpy())
    
    # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
    predictions = np.vstack(all_predictions)
    targets = np.vstack(all_targets)
    
    print(f"Generated predictions for {predictions.shape[0]} samples")
    return predictions, targets


def create_scatter_plots(predictions, ground_truth, output_dir: str):
    """å‰µå»º6è»¸æ•£é»åœ–"""
    print("Creating scatter plots...")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    plots_dir = os.path.join(output_dir, "validation_scatter_plots")
    os.makedirs(plots_dir, exist_ok=True)
    
    # 6DOF ç¶­åº¦åç¨±
    dimension_names = ['X', 'Y', 'Z', 'Roll', 'Pitch', 'Yaw']
    dimension_units = ['mm', 'mm', 'mm', 'deg', 'deg', 'deg']
    
    # è¨­å®šç¹ªåœ–é¢¨æ ¼
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    # è¨ˆç®—æ¯å€‹ç¶­åº¦çš„ RÂ² å’Œçµ±è¨ˆè³‡è¨Š
    r2_scores = []
    stats_info = {}
    
    # å‰µå»ºå€‹åˆ¥æ•£é»åœ–
    for i, (dim_name, unit) in enumerate(zip(dimension_names, dimension_units)):
        plt.figure(figsize=(10, 8))
        
        pred_dim = predictions[:, i]
        gt_dim = ground_truth[:, i]
        
        # è¨ˆç®— RÂ² åˆ†æ•¸
        r2 = r2_score(gt_dim, pred_dim)
        r2_scores.append(r2)
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        stats_info[dim_name] = {
            'r2_score': float(r2),
            'pred_mean': float(np.mean(pred_dim)),
            'pred_std': float(np.std(pred_dim)),
            'gt_mean': float(np.mean(gt_dim)),
            'gt_std': float(np.std(gt_dim)),
            'correlation': float(np.corrcoef(pred_dim, gt_dim)[0, 1])
        }
        
        # å‰µå»ºæ•£é»åœ–
        plt.scatter(gt_dim, pred_dim, alpha=0.6, s=30, edgecolors='black', linewidth=0.5)
        
        # æ·»åŠ ç†æƒ³ç·š (y=x)
        min_val = min(np.min(gt_dim), np.min(pred_dim))
        max_val = max(np.max(gt_dim), np.max(pred_dim))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction (y=x)')
        
        # æ·»åŠ è¶¨å‹¢ç·š
        z = np.polyfit(gt_dim, pred_dim, 1)
        p = np.poly1d(z)
        plt.plot(gt_dim, p(gt_dim), 'g-', linewidth=2, alpha=0.8, label=f'Trend Line (slope={z[0]:.3f})')
        
        # è¨­å®šæ¨™ç±¤å’Œæ¨™é¡Œ
        plt.xlabel(f'Ground Truth {dim_name} ({unit})', fontsize=12)
        plt.ylabel(f'Predicted {dim_name} ({unit})', fontsize=12)
        plt.title(f'{dim_name} Axis: Predicted vs Ground Truth\nRÂ² = {r2:.4f}, Correlation = {stats_info[dim_name]["correlation"]:.4f}', fontsize=14)
        
        # æ·»åŠ ç¶²æ ¼å’Œåœ–ä¾‹
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # è¨­å®šç›¸ç­‰çš„è»¸æ¯”ä¾‹
        plt.axis('equal')
        
        # ä¿å­˜å€‹åˆ¥åœ–
        plt.tight_layout()
        plt.savefig(os.path.join(plots_dir, f'scatter_{dim_name.lower()}.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… {dim_name} axis: RÂ² = {r2:.4f}")
    
    # å‰µå»ºçµ„åˆåœ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, (dim_name, unit) in enumerate(zip(dimension_names, dimension_units)):
        pred_dim = predictions[:, i]
        gt_dim = ground_truth[:, i]
        r2 = r2_scores[i]
        
        # æ•£é»åœ–
        axes[i].scatter(gt_dim, pred_dim, alpha=0.6, s=20, edgecolors='black', linewidth=0.3)
        
        # ç†æƒ³ç·š
        min_val = min(np.min(gt_dim), np.min(pred_dim))
        max_val = max(np.max(gt_dim), np.max(pred_dim))
        axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, alpha=0.8)
        
        # è¶¨å‹¢ç·š
        z = np.polyfit(gt_dim, pred_dim, 1)
        p = np.poly1d(z)
        axes[i].plot(gt_dim, p(gt_dim), 'g-', linewidth=2, alpha=0.8)
        
        # æ¨™ç±¤å’Œæ¨™é¡Œ
        axes[i].set_xlabel(f'Ground Truth {dim_name} ({unit})')
        axes[i].set_ylabel(f'Predicted {dim_name} ({unit})')
        axes[i].set_title(f'{dim_name}: RÂ² = {r2:.4f}')
        axes[i].grid(True, alpha=0.3)
        axes[i].set_aspect('equal', adjustable='box')
    
    plt.tight_layout()
    plt.savefig(os.path.join(plots_dir, 'scatter_combined.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜çµ±è¨ˆè³‡è¨Š
    stats_file = os.path.join(plots_dir, 'validation_statistics.json')
    with open(stats_file, 'w') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'overall_stats': {
                'mean_r2': float(np.mean(r2_scores)),
                'total_samples': int(len(predictions)),
                'dimensions': dimension_names
            },
            'dimension_stats': stats_info
        }, f, indent=2)
    
    print(f"ğŸ“Š Scatter plots saved to: {plots_dir}")
    print(f"ğŸ“„ Statistics saved to: {stats_file}")
    print(f"ğŸ¯ Overall mean RÂ²: {np.mean(r2_scores):.4f}")
    
    return r2_scores, stats_info


def main():
    parser = argparse.ArgumentParser(description="Generate validation scatter plots from trained model")
    parser.add_argument("--checkpoint_path", type=str, required=True, 
                       help="Path to model checkpoint (.ckpt file)")
    parser.add_argument("--data_dir", type=str, default="data/processed",
                       help="Data directory path")
    parser.add_argument("--output_dir", type=str, default="validation_analysis",
                       help="Output directory for plots")
    parser.add_argument("--batch_size", type=int, default=16,
                       help="Batch size for inference")
    parser.add_argument("--use_custom_split", action="store_true",
                       help="Use custom split where patients 1-5 are validation set")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥ checkpoint æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.checkpoint_path):
        print(f"âŒ Checkpoint file not found: {args.checkpoint_path}")
        return
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("ğŸš€ Starting validation analysis...")
    print(f"ğŸ“ Checkpoint: {args.checkpoint_path}")
    print(f"ğŸ“ Data directory: {args.data_dir}")
    print(f"ğŸ“ Output directory: {args.output_dir}")
    
    try:
        # 1. è¼‰å…¥æ¨¡å‹
        model = load_model_from_checkpoint(args.checkpoint_path)
        
        # 2. å‰µå»ºé©—è­‰é›†
        val_dataset = create_validation_dataset(args.data_dir, args.use_custom_split)
        
        # å‰µå»º DataLoader
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=False,
            persistent_workers=False
        )
        
        # 3. ç”Ÿæˆé æ¸¬
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        predictions, ground_truth = generate_predictions(model, val_loader, device)
        
        # 4. å‰µå»ºæ•£é»åœ–
        r2_scores, stats_info = create_scatter_plots(predictions, ground_truth, args.output_dir)
        
        # 5. æ‰“å°ç¸½çµ
        print("\nğŸ‰ Analysis completed successfully!")
        print("\nğŸ“Š Final Results:")
        dimension_names = ['X', 'Y', 'Z', 'Roll', 'Pitch', 'Yaw']
        for i, dim_name in enumerate(dimension_names):
            print(f"  {dim_name:5s}: RÂ² = {r2_scores[i]:.4f}")
        print(f"  Mean : RÂ² = {np.mean(r2_scores):.4f}")
        
        print(f"\nğŸ“ All plots saved to: {args.output_dir}/validation_scatter_plots/")
        
    except Exception as e:
        print(f"âŒ Error during analysis: {e}")
        raise


if __name__ == "__main__":
    main() 